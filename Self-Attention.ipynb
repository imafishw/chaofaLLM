{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "class selfAttentionV1(nn.Module):\n",
    "  def __init__(self, hidden_dim:int = 728) ->None:\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    # 初始化三个不同的线性映射层\n",
    "    self.W_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.W_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.W_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "  def forward(self, X):\n",
    "    # X: [batch_size, seq_len, hidden_dim]\n",
    "    Q = self.W_q(X)\n",
    "    K = self.W_k(X)\n",
    "    V = self.W_v(X)\n",
    "    # Q, K, V: [batch_size, seq_len, hidden_dim]\n",
    "    # 计算注意力分数\n",
    "    attention_value = torch.matmul(\n",
    "      Q,K.transpose(-1, -2)\n",
    "    )\n",
    "    # attention_value: [batch_size, seq_len, seq_len]\n",
    "    attention_weight = torch.softmax(\n",
    "      attention_value / math.sqrt(self.hidden_dim),\n",
    "      dim=-1\n",
    "    )\n",
    "    print(attention_weight)\n",
    "    # 需要对softmax的最后一个维度做softmax A = K^T * Q\n",
    "    # A 矩阵的横排进行了 softmax运算\n",
    "    # (bacth,seq ,hidden) * (bacth, seq, hidden)\n",
    "    attention_output = torch.matmul(\n",
    "      attention_weight, V\n",
    "    )\n",
    "    return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3802,  0.4722,  0.1428,  0.1132],\n",
       "         [ 0.5309,  0.4243,  0.3981, -0.0057]],\n",
       "\n",
       "        [[ 0.5544,  1.2497, -0.9296,  1.0535],\n",
       "         [ 0.2638,  0.4064,  0.1934,  2.0676]],\n",
       "\n",
       "        [[ 1.9152,  0.3164,  0.6095,  0.5894],\n",
       "         [ 1.9456,  1.9285,  1.0518,  0.9114]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(3,2,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4917, 0.5083],\n",
      "         [0.4919, 0.5081]],\n",
      "\n",
      "        [[0.5943, 0.4057],\n",
      "         [0.5051, 0.4949]],\n",
      "\n",
      "        [[0.4151, 0.5849],\n",
      "         [0.3315, 0.6685]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9084,  0.6090,  0.0637, -0.0840],\n",
       "         [ 0.9084,  0.6089,  0.0637, -0.0840]],\n",
       "\n",
       "        [[ 0.8165,  0.7386, -0.3221, -1.1168],\n",
       "         [ 0.7677,  0.7507, -0.3931, -1.1077]],\n",
       "\n",
       "        [[ 1.9613,  1.4878,  0.4216,  0.0239],\n",
       "         [ 2.0266,  1.5347,  0.4132, -0.0323]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_atn = selfAttentionV1(4)\n",
    "self_atn(X)\n",
    "# 2*4 * 4*2 # 2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout\n",
    "# attention_mask\n",
    "# output 矩阵映射\n",
    "class selfAttentionV3(nn.Module):\n",
    "  def __init__(self, hidden_dim:int = 728,dropout_rate = 0.1,*args,**kwargs) ->None:    \n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.hidden_dim = hidden_dim\n",
    "    # 初始化三个不同的线性映射层\n",
    "    self.proj = nn.Linear(hidden_dim, hidden_dim*3)\n",
    "    self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "    self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "  def forward(self, X ,attention_mask = None):\n",
    "    QKV = self.proj(X)\n",
    "    Q,K,V = torch.split(QKV, self.hidden_dim, dim=-1)\n",
    "    attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "    if attention_mask is not None:\n",
    "      attention_weight = attention_weight.masked_fill(\n",
    "        attention_mask==0,\n",
    "        float(\"-1e20\")\n",
    "      )\n",
    "    print(attention_weight)\n",
    "    attention_weight = torch.softmax(\n",
    "      attention_weight, dim=-1\n",
    "    )\n",
    "    attention_weight = self.attention_dropout(attention_weight)\n",
    "    attention_result = attention_weight @ V\n",
    "    #\n",
    "    output = self.output_proj(attention_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(3,2,4)\n",
    "mask = torch.tensor(\n",
    "  [\n",
    "    [1,1,1,0],\n",
    "    [1,1,0,0],\n",
    "    [1,0,0,0]\n",
    "  ]\n",
    ")\n",
    "mask = mask.unsqueeze(dim=1).repeat(1,4,1)\n",
    "print(\"repeat shape is:{mask.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn about some fuction from pytorch\n",
    "'''\n",
    "like a mask matrix\n",
    "[\n",
    "  1,2,3       \n",
    "  4,5,6   \n",
    "  7,8,9\n",
    "]\n",
    "mask matrix \n",
    "[\n",
    "  1 1 1\n",
    "  0 1 1\n",
    "  0 0 1\n",
    "]\n",
    "在经过Mask矩阵的乘法后0的位置(False)会被置为0 \n",
    "[\n",
    "  1 2 3\n",
    "  0 5 6\n",
    "  0 0 9\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False],\n",
      "        [False,  True, False, False],\n",
      "        [False, False,  True, False],\n",
      "        [False, False, False,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  0,  6,  7],\n",
       "        [ 8,  9,  0, 11],\n",
       "        [12, 13, 14,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(0, 16).view(4, 4)\n",
    "mask = torch.eye(4, dtype=torch.bool)\n",
    "print(mask)\n",
    "masked_X = X.masked_fill(mask, 0)\n",
    "masked_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[ 1.0157,  1.3349,  0.2062, -1.1796, -0.5577],\n",
      "        [-0.4872, -1.5037, -1.1942,  0.0972,  0.1060],\n",
      "        [ 0.6175, -0.3676, -0.3595,  1.3189, -0.2613],\n",
      "        [ 2.2171, -0.8076,  1.6101,  0.1074,  1.2273],\n",
      "        [-0.0859,  0.1804, -1.2247,  1.0781, -0.7553]])\n",
      "Output Tensor after Dropout:\n",
      " tensor([[ 1.2696,  1.6687,  0.2577, -1.4745, -0.6972],\n",
      "        [-0.6090, -1.8797, -1.4928,  0.1215,  0.1325],\n",
      "        [ 0.0000, -0.4595, -0.4493,  0.0000, -0.0000],\n",
      "        [ 2.7714, -0.0000,  2.0126,  0.0000,  1.5341],\n",
      "        [-0.1073,  0.0000, -1.5309,  1.3476, -0.9441]])\n"
     ]
    }
   ],
   "source": [
    "# Dropout 是一种常用的正则化方法 随机将部分神经元的输出置为0\n",
    "m = nn.Dropout(p=0.2)  # p是丢弃的概率\n",
    "input_tensor = torch.randn(5, 5)\n",
    "output_tensor = m(input_tensor)\n",
    "print(\"Input Tensor:\\n\", input_tensor)\n",
    "print(\"Output Tensor after Dropout:\\n\", output_tensor)\n",
    "# 部分变为0 占总数的0.2\n",
    "# 非0参数都除以0.8这些值都变大了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor2 = torch.tensor([[4,5],\n",
    "                        [4,5],\n",
    "                        [4,5]])\n",
    "# matmul函数带有广播机制 适用于高维向量的矩阵乘法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
