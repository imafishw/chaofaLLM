# ğŸ§  PyTorch ä¸­çº¿æ€§å˜æ¢ï¼ˆ`nn.Linear`ï¼‰ä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„çŸ©é˜µä¹˜æ³•ç†è§£

## ğŸ“Œ é—®é¢˜èƒŒæ™¯

åœ¨ Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé€šå¸¸ä¼šä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```python
self.W_q = nn.Linear(hidden_dim, hidden_dim)
Q = self.W_q(X)  # X shape: [batch_size, seq_len, hidden_dim]
```

åˆå­¦è€…å¸¸æœ‰ç–‘é—®ï¼š
> è¾“å…¥ `X` æ˜¯ä¸‰ç»´çš„ `[B, S, D]`ï¼Œè€Œæƒé‡çŸ©é˜µæ˜¯äºŒç»´çš„ `[D, D]`ï¼Œå®ƒä»¬æ€ä¹ˆç›¸ä¹˜ï¼ŸPyTorch æ˜¯ä¸æ˜¯è‡ªåŠ¨æ‹†åˆ†äº† tokenï¼Ÿ

---

## âœ… ç­”æ¡ˆæ€»ç»“

**PyTorch ä¸ä¼šæ˜¾å¼åœ°â€œæ‹†åˆ†â€ token**ï¼Œä½†å®ƒä¼šè‡ªåŠ¨è¯†åˆ«è¾“å…¥å¼ é‡çš„ **æœ€åä¸€ä¸ªç»´åº¦ä¸ºç‰¹å¾ç»´åº¦**ï¼Œå¹¶å¯¹è¯¥ç»´åº¦æ‰§è¡Œçº¿æ€§å˜æ¢ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼š

- è¾“å…¥ï¼š`[batch_size, seq_len, hidden_dim]`
- æƒé‡çŸ©é˜µï¼š`[hidden_dim, hidden_dim]`
- è¾“å‡ºï¼š`[batch_size, seq_len, hidden_dim]`

è¿™ä¸ªè¿‡ç¨‹ç­‰ä»·äºå¯¹æ¯ä¸ª token å‘é‡åˆ†åˆ«è¿›è¡Œä¸€æ¬¡çº¿æ€§å˜æ¢ã€‚

---

## ğŸ” å…·ä½“åŸç†

### 1. `nn.Linear` çš„å·¥ä½œæœºåˆ¶

```python
layer = nn.Linear(in_features, out_features)
output = layer(input)
Q = self.W_q(X)  # X.shape = [batch_size, seq_len, hidden_dim]
```  


Q = self.W_q(X)  # X.shape = [batch_size, seq_len, hidden_dim]ç­‰ä»·äºä¸‹é¢ä»£ç 
```python
for i in range(batch_size):
    for j in range(seq_len):
        Q[i, j] = W_q(X[i, j])
```

- `nn.Linear` å§‹ç»ˆä½œç”¨äº **è¾“å…¥å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦**
- å®ƒæ”¯æŒä»»æ„ç»´åº¦çš„è¾“å…¥ï¼Œä¸é™äºäºŒç»´
- æ‰€ä»¥å³ä½¿è¾“å…¥æ˜¯ä¸‰ç»´ `[B, S, D]`ï¼Œå®ƒä¹Ÿä¼šè‡ªåŠ¨å¯¹æ¯ä¸ª `[D]` å‘é‡åšå˜æ¢

### 2. çŸ©é˜µä¹˜æ³•ç¤ºä¾‹

è®¾ï¼š
- `X.shape = [1, 3, 4]`ï¼ˆbatch=1, seq_len=3, hidden_dim=4ï¼‰
- `W_q.shape = [4, 4]`

åˆ™ï¼š

```python
Q = W_q(X)  # Q.shape = [1, 3, 4]
```

ç›¸å½“äºå¯¹æ¯ä¸ª token å‘é‡åšäº†ï¼š

```python
Q_i = X_i @ W_q.T + b_q
```

---

## ğŸ“ˆ æ•°å­¦è¡¨è¾¾

è®¾è¾“å…¥å¼ é‡ï¼š

$$
X \in \mathbb{R}^{B \times S \times D}
$$

æƒé‡çŸ©é˜µï¼š

$$
W_q \in \mathbb{R}^{D \times D}
$$

è¾“å‡ºå¼ é‡ï¼š

$$
Q = X W_q^T + b_q \Rightarrow Q \in \mathbb{R}^{B \times S \times D}
$$

---

## ğŸ§ª å®ç°æ–¹å¼å¯¹æ¯”

### æ–¹å¼ä¸€ï¼šç›´æ¥ä½¿ç”¨ `nn.Linear`

```python
Q = W_q(X)  # æ¨èåšæ³•ï¼Œç®€æ´é«˜æ•ˆ
```

### æ–¹å¼äºŒï¼šæ‰‹åŠ¨ reshape æˆäºŒç»´å†è®¡ç®—

```python
B, S, D = X.shape
X_flat = X.view(B * S, D)
Q_flat = W_q(X_flat)
Q = Q_flat.view(B, S, D)
```

ä¸¤ç§æ–¹å¼ç»“æœä¸€è‡´ï¼Œæ€§èƒ½ä¸ŠåŸºæœ¬æ²¡æœ‰å·®åˆ«ã€‚

---

## ğŸ§  æ€»ç»“ä¸€å¥è¯

> **PyTorch åœ¨æ‰§è¡Œ `nn.Linear` æ—¶ä¸ä¼šæ˜¾å¼æ‹†åˆ† tokenï¼Œè€Œæ˜¯å¯¹è¾“å…¥å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œè‡ªåŠ¨å®ç°å¯¹æ¯ä¸ª token çš„çº¿æ€§å˜æ¢ã€‚**

---

## ğŸ’¡ æ‹“å±•å­¦ä¹ å»ºè®®

å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œè¿˜å¯ä»¥ç»§ç»­å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š
- è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ Q/K/V çš„å…·ä½“ç”¨é€”
- å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰çš„å·¥ä½œåŸç†
- ä½¿ç”¨ `torch.einsum` æ›´æ¸…æ™°åœ°è¡¨è¾¾å¤šç»´è¿ç®—
- å¦‚ä½•å¯è§†åŒ– token å‘é‡å’Œ attention åˆ†å¸ƒ

---


# ğŸ¯ å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ç®€ä»‹ä¸ç®€å•å®ç°

## ğŸ¤” ä»€ä¹ˆæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ

å¤šå¤´æ³¨æ„åŠ›æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ã€‚å®ƒçš„æ€æƒ³æ˜¯ï¼š

- å°†è¾“å…¥æ˜ å°„åˆ°å¤šä¸ªä¸åŒçš„å­ç©ºé—´ï¼ˆå³å¤šä¸ªâ€œå¤´â€ï¼‰
- æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®— attention score å’ŒåŠ æƒæ±‚å’Œ
- æœ€åå°†å„ä¸ªå¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ•´åˆ

è¿™æ ·åšçš„å¥½å¤„æ˜¯æ¨¡å‹å¯ä»¥ä»ä¸åŒè§’åº¦æå–ä¿¡æ¯ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›ã€‚

---

## ğŸ“ è¾“å…¥è¾“å‡ºå®šä¹‰

å‡è®¾ï¼š

- `batch_size = B`
- `seq_len = S`
- `hidden_dim = D`
- `num_heads = H`
- æ¯ä¸ª head çš„ç»´åº¦ä¸ºï¼š`d_k = D // H`

---

## ğŸ§± ç®€å•å®ç°æ­¥éª¤

æˆ‘ä»¬æ¥æ„å»ºä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›æ¨¡å—çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå¿½ç•¥ softmaxã€masking ç­‰ç»†èŠ‚ï¼‰ï¼š

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.d_k = hidden_dim // num_heads
        
        # çº¿æ€§æŠ•å½±å±‚
        self.W_q = nn.Linear(hidden_dim, hidden_dim)
        self.W_k = nn.Linear(hidden_dim, hidden_dim)
        self.W_v = nn.Linear(hidden_dim, hidden_dim)
        self.W_o = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, X):
        batch_size, seq_len, _ = X.size()
        
        # çº¿æ€§å˜æ¢
        Q = self.W_q(X)
        K = self.W_k(X)
        V = self.W_v(X)

        # åˆ†å¤´ï¼š[B, S, H, d_k] -> [B, H, S, d_k]
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # è®¡ç®— attention scoresï¼ˆç®€åŒ–ç‰ˆï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn, V)

        # åˆå¹¶å¤´ï¼š[B, H, S, d_k] -> [B, S, H*d_k]
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)

        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_o(context)
        
        return output
```

---

## ğŸ§ª ç¤ºä¾‹ä½¿ç”¨

```python
model = MultiHeadAttention(hidden_dim=768, num_heads=8)
X = torch.randn(2, 10, 768)  # batch_size=2, seq_len=10, hidden_dim=768
output = model(X)
print(output.shape)  # [2, 10, 768]
```

---

## ğŸ“Š å¯è§†åŒ–ç†è§£ï¼ˆä¼ªä»£ç ï¼‰

ä½ å¯ä»¥æŠŠæ•´ä¸ªæµç¨‹æƒ³è±¡æˆï¼š

```
Input X: [B, S, D]
   â†“
Linear: [B, S, D] â†’ [B, S, D] (for Q, K, V)
   â†“
Split into heads: [B, H, S, d_k]
   â†“
Compute attention: for each head
   â†“
Concatenate heads: [B, S, D]
   â†“
Final linear: [B, S, D]
```

---

## ğŸ’¡ æ€»ç»“

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| `W_q`, `W_k`, `W_v` | æŠ•å½±åˆ° Queryã€Keyã€Value ç©ºé—´ |
| åˆ†å¤´ï¼ˆSplitï¼‰ | å°†å‘é‡åˆ†å‰²ä¸ºå¤šä¸ª head |
| attention score | è®¡ç®— token ä¹‹é—´çš„ç›¸å…³æ€§ |
| åŠ æƒæ±‚å’Œ | ä½¿ç”¨ attention æƒé‡èšåˆä¿¡æ¯ |
| åˆå¹¶å¤´ï¼ˆConcatï¼‰ | å°†å¤šä¸ª head çš„ç»“æœåˆå¹¶ |
| `W_o` | æœ€ç»ˆæ•´åˆè¾“å‡º |

å¦‚ä¸‹å›¾æ‰€ç¤º åªå…³æ³¨æœ€åä¸¤ä¸ªç»´åº¦ å…¶ä»–çš„ ç®—æ˜¯å¹¿æ’­ ç±»ä¼¼äºåµŒå¥—å¾ªç¯
---
      Q: [B, H, S, D]       Ã—       K^T: [B, H, D, S]
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”œâ”€(matrix mul)                  â”œâ”€(dim swap)
                 â†“                               â†“
           result: [B, H, S, S]
## ğŸ“š æ‹“å±•å­¦ä¹ å»ºè®®

å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œè¿˜å¯ä»¥ç»§ç»­å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š
- å¦‚ä½•åŠ å…¥ maskï¼ˆpadding mask æˆ– causal maskï¼‰
- `torch.nn.MultiheadAttention` çš„ä½¿ç”¨
- è‡ªæ³¨æ„åŠ› vs äº¤å‰æ³¨æ„åŠ›çš„åŒºåˆ«
- ä½¿ç”¨ `einops` æ›´ä¼˜é›…åœ°å¤„ç†å¤šç»´å¼ é‡å˜æ¢

---

å¦‚éœ€æˆ‘å¸®ä½ ç»§ç»­æ‰©å±•è¿™ä»½ç¬”è®°ï¼Œæ¯”å¦‚åŠ å…¥å›¾è§£ã€åŠ¨ç”»æ¼”ç¤ºã€å¯è§†åŒ– attention åˆ†å¸ƒç­‰å†…å®¹ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ ğŸ˜Š